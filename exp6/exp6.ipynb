{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131c6d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "WARNING:tensorflow:From D:\\apps\\anaconda\\envs\\dlc\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Found 25000 files belonging to 2 classes.\n",
      "b\"Sorry to say I have no idea what Hollywood is doing. Sure give us movies like Batman Begins. Oh, by the way Hollywood I think they may cover the story line in the movie Batman, but please don't entertain us what we would really want to see Batman and Superman together. I really hated this trailer because it left me wanting for more. I was looking around to see when it was coming out. It was like a terrible practical joke. The graphics where good the story line seemed solid and it had all the trappings of a great movie. Unfortunately it's not going to happen for now. To the producers, directors and all the actors great job but I hate you for doing this to me. You left me wanting more.\"\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )\n",
    "\n",
    "# Assumes you're in the root level of the dataset directory.\n",
    "# If you aren't, you'll need to change the relative paths here.\n",
    "train_data = prepareData('./datasets/train')\n",
    "test_data = prepareData('./datasets/test')\n",
    "\n",
    "for text_batch, label_batch in train_data.take(1):\n",
    "  print(text_batch.numpy()[0])\n",
    "  print(label_batch.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69239219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# ----- 1. INPUT\n",
    "# We need this to use the TextVectorization layer next.\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "\n",
    "# ----- 2. TEXT VECTORIZATION\n",
    "# This layer processes the input string and turns it into a sequence of\n",
    "# max_len integers, each of which maps to a certain token.\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  # Max vocab size. Any words outside of the max_tokens most common ones\n",
    "  # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
    "  max_tokens=max_tokens,\n",
    "  # Output integer indices, one per string token\n",
    "  output_mode=\"int\",\n",
    "  # Always pad or truncate to exactly this many tokens\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "# Call adapt(), which fits the TextVectorization layer to our text dataset.\n",
    "# This is when the max_tokens most common words (i.e. the vocabulary) are selected.\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# ----- 3. EMBEDDING\n",
    "# This layer turns each integer (representing a token) from the previous layer\n",
    "# an embedding. Note that we're using max_tokens + 1 here, since there's an\n",
    "# out-of-vocabulary (OOV) token that gets added to the vocab.\n",
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "\n",
    "# ----- 4. RECURRENT LAYER\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# ----- 5. DENSE HIDDEN LAYER\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# ----- 6. OUTPUT\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0785f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 67s 69ms/step - loss: 0.5394 - accuracy: 0.7262\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 44s 57ms/step - loss: 0.4418 - accuracy: 0.7976\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.4064 - accuracy: 0.8168\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3807 - accuracy: 0.8310\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.3643 - accuracy: 0.8412\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 49s 63ms/step - loss: 0.3456 - accuracy: 0.8502\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 42s 53ms/step - loss: 0.3271 - accuracy: 0.8605\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 42s 54ms/step - loss: 0.3144 - accuracy: 0.8660\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 49s 62ms/step - loss: 0.3052 - accuracy: 0.8715\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 44s 55ms/step - loss: 0.2961 - accuracy: 0.8746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1f3556d5fc8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_data, epochs=10)\n",
    "\n",
    "model.save_weights('rnn')\n",
    "\n",
    "model.load_weights('rnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b725212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 55s 67ms/step - loss: 0.5624 - accuracy: 0.7828\n",
      "1/1 [==============================] - 1s 693ms/step\n",
      "[[0.9934685]]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[[0.01894102]]\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_data)\n",
    "\n",
    "# Should print a very high score like 0.98.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5f9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
